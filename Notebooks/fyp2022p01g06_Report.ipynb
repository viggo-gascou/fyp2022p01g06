{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project - Project 1 - Corona and Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Group 6 (F)**: Bjørn Søvad (bjso@itu.dk), Katarina Kraljevic (katkr@itu.dk), Mirka Katuscáková (katu@itu.dk), Emma Cecilie Bjerring Jensen (emcj@itu.dk), Viggo Yann Unmack Gascou (viga@itu.dk)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for running this Jupyter Notebook\n",
    "Make sure to run this notebook in trusted mode. \\\n",
    "This will ensure that the code blocks run correctly and display the output correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected folder structure. In order for this notebook to run correctly this folder structure must be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "fyp2022p01g06/\n",
    "├─ Notebooks/\n",
    "│  ├─ fyp2022p01g06_Report.ipynb (current location)\n",
    "├─ Data/\n",
    "│  ├─ raw/\n",
    "│  │  ├─ corona/\n",
    "│  │  ├─ weather/\n",
    "│  │  ├─ shapefiles/\n",
    "│  │  ├─ metadata/\n",
    "│  ├─ external/\n",
    "│  │  ├─ de_our_world_in_data_covid.csv\n",
    "│  │  ├─ de_subnational_HDI.csv\n",
    "│  ├─ processed/\n",
    "│  │  ├─ ...\n",
    "│  │  interim/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries\n",
    "If needed you can read more documentation about the different libraries that are imported\n",
    "* [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "* [Numpy Documentation](https://numpy.org/doc/)\n",
    "* [Folium Documentation](https://python-visualization.github.io/folium/)\n",
    "* [Json Documentation](https://docs.python.org/3/library/json.html)\n",
    "* [Statsmodels Documentation](https://www.statsmodels.org/stable/)\n",
    "* [Scipy Documentation](https://scipy.github.io/devdocs/index.html)\n",
    "* [Matplotlib Documentation](http://matplotlib.org/)\n",
    "* [Seaborn Documentation](http://seaborn.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing needed libraries\n",
    "import pandas as pd                                                            # used to provide major datastructure pd.DataFrame() to store the datasets\n",
    "import numpy as np                                                             # used for numerical calculations and fast array manipulations\n",
    "import folium                                                                  # used for spatial data visualizations\n",
    "import json                                                                    # used for loading json data correctly\n",
    "import statsmodels.api as sm                                                   # used to run multivariate linear regression\n",
    "from scipy.stats import pearsonr, spearmanr                                    # used to run 'pearson' and 'spearman' association tests of numerical variables on two variables\n",
    "from statsmodels.stats.multitest import multipletests                          # used to run multiple tests of p-values for multiple variables\n",
    "import matplotlib.pyplot as plt                                                # used for plotting and visualizing our data\n",
    "import matplotlib.dates as mdates                                              # used to create right ticks for x-axes in diagrams\n",
    "import seaborn as sns                                                          # used for plotting and visualizing our data\n",
    "from IPython.display import Markdown, display                                  # used to print stuff with markdown/HTML formatting for bold text and colored text\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the raw corona data from Germany\n",
    "corona_df = pd.read_csv('../data/raw/corona/de_corona.csv', sep = '\\t')\n",
    "corona_df.name = 'corona_df'\n",
    "\n",
    "#Importing the raw weather data for the countries, Germany, Netherlands, Sweden and Denmark\n",
    "weather_df = pd.read_csv(\"../data/raw/weather/weather.csv\")\n",
    "weather_df.name = 'weather_df'\n",
    "\n",
    "#Loading in the metadata json using the Python json library\n",
    "with open('../data/raw/metadata/de_metadata.json','r', encoding=\"utf8\") as f:\n",
    "    country_metadata=json.load(f)\n",
    "\n",
    "#Creating a folium map (called de_map) that is based around Germany and uneditable in terms of placement and zoom\n",
    "de_map = folium.Map(location = [51.1657, 10.4515], zoom_start = 6, crs = 'EPSG3857', \n",
    "    zoom_control = False, scrollWheelZoom = False, dragging = False)\n",
    "\n",
    "#Loading in the geojson that contains data for the regions and borders of Germany and adding it to the folium map\n",
    "folium.GeoJson('../data/raw/shapefiles/de.geojson', name = \"geojson\").add_to(de_map)\n",
    "folium.LayerControl().add_to(de_map);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Data filtering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analysis done in this notebook is done with a handful of different datasets:\n",
    "\n",
    "> CSV: Corona (DE) - Contains the Number of new infections (per day) and Number of new casualties (per day) filtered by day and region in Germany for each day in the period `2020-01-02` to `2021-02-21`.\n",
    ">\n",
    "> CSV: Weather - Contains information about several indicators of weather conditions for each region in Germany, Denmark, Sweden and Netherlands for each day in the period `2020-02-13` to `2021-02-21`\n",
    ">\n",
    "> JSON: Metadata (DE) - Contains more information on the population in the different regions in Germany\n",
    ">\n",
    "> GEOJSON: Geojson (DE) - Holds the geojson data for the different regions in Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial inspection of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, cols = weather_df.shape\n",
    "print(\"Number of Rows: \" + str(row))\n",
    "print(\"Number of Columns: \" + str(cols))\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `weather` dataset contains `9` different variables for the countries Germany, Denmark Sweden and Netherlands spread across `20220` rows and `9` columns. Each row contains `7` different weather measurements for a particular day in a specific country and region:\n",
    "> `Date` (YYYY-DD-MM): Is the day of which the weather measurements were made and reported\n",
    ">\n",
    "> `Iso3166-2`: Is the [ISO 3166-2](https://en.wikipedia.org/wiki/ISO_3166-2) code for the region for which the weather measurements were made\n",
    ">\n",
    "> `Relative Humidity Surface` (%): Is the **daily average** relative humidity of the surface\n",
    ">\n",
    "> `Solar Radiation`(W/m^2): Is the **daily sum** of the solar radiation from the sun\n",
    ">\n",
    "> `Surface Pressure` (Pa): Is the **daily sum** of the atmospheric pressure at the surface of the earth\n",
    ">\n",
    "> `TemperatureAboveGround` (º K): Is the **daily average** of the temperature above ground\n",
    ">\n",
    "> `Total Precipitation` (mm): Is the **daily sum** of the total precipitation \n",
    ">\n",
    "> `UV Index` (Numerical value): Is the **daily sum** of the strength of the ultraviolet radiation\n",
    ">\n",
    "> `Wind Speed`(m/s): Is the **daily average** of the wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, cols = corona_df.shape\n",
    "print(\"Number of Rows: \" + str(row))\n",
    "print(\"Number of Columns: \" + str(cols))\n",
    "corona_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `corona` dataset contains `4` different variables for corona reports for Germany spread across `5602` rows and `4` columns. Each row contains `2` different corona reports for a particular day in a specific country and region:\n",
    "> `Date` (YYYY-DD-MM): Is the day of which the corona report was made\n",
    ">\n",
    "> `Region Code`: Is the region code for the region for which the corona report was made\n",
    ">\n",
    "> `Confirmed Addition`: Is the number of new confirmed infections of corona for the specific day in the specified region\n",
    ">\n",
    "> `Deceased Addition`: Is the number of newly confirmed deaths for the specific day in the specified region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Checking to see if there are any missing (Na or NaN) values in either of the two datasets \\\n",
    "\\\n",
    "We also check if there are any duplicate rows in either of the two datasets. \\\n",
    "\\\n",
    "Next we also check if any of the reported numerical values are negative. It would not make sense to e.g., have a negative UV Index or a negative comfirmed COVID-19 cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [weather_df, corona_df]\n",
    "for dataset in datasets:\n",
    "    if dataset.name == \"weather_df\":\n",
    "        printmd(\"### **Weather Dataset**\")\n",
    "    else: \n",
    "        printmd(\"### **Corona Dataset**\")\n",
    "\n",
    "    print(dataset.isnull().any())\n",
    "    print(\"-----------------------------------------------\")\n",
    "    printmd( \"##### <span style='color:red'> ❌ **Oh No! There are missing data values in the dataset!**</span>\" if dataset.isnull().any().any() \n",
    "            else\n",
    "            \"##### <span style='color:lightgreen'> ✅ **Great! There are no missing data values in the dataset!**</span>\")\n",
    "    printmd(\"____\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    if dataset.name == \"weather_df\":\n",
    "        printmd(\"### **Weather Dataset**\")\n",
    "    else: \n",
    "        printmd(\"### **Corona Dataset**\")\n",
    "\n",
    "    print(dataset.duplicated())\n",
    "    print(\"-----------------------------------------------\")\n",
    "    printmd( \"##### <span style='color:red'> ❌ **Oh No! There are duplicate data values in the dataset!**</span>\" if dataset.duplicated().any()\n",
    "            else\n",
    "            \"##### <span style='color:lightgreen'> ✅ **Great! There are no duplicate data values in the dataset!**</span>\")\n",
    "    printmd(\"____\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for values negative reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    if dataset.name == \"weather_df\":\n",
    "        printmd(\"### **Weather Dataset**\")\n",
    "    else: \n",
    "        printmd(\"### **Corona Dataset**\")\n",
    "    dataset = dataset.select_dtypes(include=np.number)\n",
    "\n",
    "    print((dataset < 0).any())\n",
    "    print(\"-----------------------------------------------\")\n",
    "    printmd( \"##### <span style='color:red'> ❌ **Oh No! There are data values that shouldn't be negative in the dataset!**</span>\" if (dataset < 0).any().any() \n",
    "            else\n",
    "            \"##### <span style='color:lightgreen'> ✅ **Great! There are no data values that shouldn't be negative in the dataset!**</span>\")\n",
    "    printmd(\"____\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing variable names\n",
    "\n",
    "We noticed that the variables in the weather dataset had a different naming scheme than the corona dataset so in order to make it more seamless we rename the variables to all have the same naming scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.rename(columns={\n",
    "    'date': 'date', \n",
    "    'iso3166-2': 'region_code',\n",
    "    'RelativeHumiditySurface': 'relative_humidity_surface', \n",
    "    'SolarRadiation': 'solar_radiation', \n",
    "    'Surfacepressure': 'surface_pressure', \n",
    "    'TemperatureAboveGround': 'temperature_above_ground', \n",
    "    'Totalprecipitation': 'total_precipitation', \n",
    "    'UVIndex': 'UV_index', \n",
    "    'WindSpeed': 'wind_speed'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no missing or falsely reported data values we can move on to cleaning and filtering the data.\n",
    "\n",
    "#### _Let's start by cleaning and filtering the corona data for Germany:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start of by creating to dictionaries that are going to help us to map both regions and their populations to the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary that contains the full names of the different regions as keys and their respective iso3166-2 code as values\n",
    "region_map = {country_metadata[\"country_metadata\"][i][\"covid_region_code\"]: \n",
    "    country_metadata[\"country_metadata\"][i][\"iso3166-2_code\"] for i in range(len(country_metadata[\"country_metadata\"]))}\n",
    "\n",
    "#Creating a dictionary that contains the full names of the different regions as keys and their respective populations as values\n",
    "population_map = {country_metadata[\"country_metadata\"][i][\"iso3166-2_code\"]: \n",
    "    country_metadata[\"country_metadata\"][i][\"population\"] for i in range(len(country_metadata[\"country_metadata\"]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the region_map we can create extra columns to add the iso3166-2 code to the corona dataframe as it only contained to full region codes. \\\n",
    "\\\n",
    "We can also use the population_map to add a column with the population of each of the regions to the corona dataframe. \\\n",
    "\\\n",
    "With the population column we can create a column that contains the cases and deaths per capita using the confirmed addition and the population column we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the region_map dictionary to create a new column with the respective iso3166-2 code for each region based on the full region name\n",
    "#from the region_code column\n",
    "corona_df[\"region_code\"] = corona_df[\"region_code\"].map(region_map)\n",
    "\n",
    "#Using the population_map dictionary to create a new column with the respective population for each region based on the iso3166-2 code\n",
    "#from the iso3166-2 column\n",
    "corona_df[\"population\"] = corona_df[\"region_code\"].map(population_map)\n",
    "\n",
    "#Also adding a cases and deaths per capita column that is created using the confirmed covid cases and deaths respectively divided by the population in that region\n",
    "corona_df[\"cases_pc\"] = corona_df[\"confirmed_addition\"] / corona_df[\"population\"]\n",
    "corona_df[\"deaths_pc\"] = corona_df[\"deceased_addition\"] / corona_df[\"population\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Now moving on to the weather dataset:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by first filtering out all the other countries from the dataset as we are not interested in their data.\\\n",
    "\\\n",
    "We can then convert the temperature measurements from º K to º C, we do that by subtracting `273.15` from each row value in the temperature measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:IndianRed'>Please note! Be careful to not run the following cell more than once as this will keep doing the subtractions which could lead to negative and or NaN temperature values</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out all the weather data that is not relevant as we are only interested in weatherdata from Germany\n",
    "weather_df = weather_df[weather_df[\"region_code\"].str.startswith(\"DE\")]\n",
    "\n",
    "#Converting the temperature from Kelvin to Celsius\n",
    "weather_df[\"temperature_above_ground\"] = weather_df[\"temperature_above_ground\"] - 273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can merge the corona_df and the weather_df to create on big dataframe/dataset that contains all the data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the weatherdata with the coronadata to create one dataframe with all the data that we need\n",
    "merged_df = corona_df.merge(weather_df, on = [\"date\", \"region_code\"])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have merged the two datasets we want to check how many rows we lost from each of the two datasets. \\\n",
    "\\\n",
    "We do that by subtracting the number of rows from each of the two datasets by the number of rows in the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_lost = weather_df.shape[0]-merged_df.shape[0]\n",
    "corona_lost = corona_df.shape[0]-merged_df.shape[0]\n",
    "print(f\"We lost {weather_lost} rows from the weather dataset.\")\n",
    "print(f\"We lost {corona_lost} rows from the corona datset\")\n",
    "print(f\"We lost {weather_lost+corona_lost} rows in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we lost 442 rows from the two datasets by merging.\n",
    "This could be due to the fact that the corona dataset starts from the 2nd of January \\\n",
    "whereas the weather dataset starts on the 13th of February and therefore some of the corona data is lost. \\\n",
    "It could also be possible that on the days that do match one of the datasets does not contain data on the same region as the other dataset, resulting in more dropped rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting and re-loading filtered and cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now having cleaned and filtered to two datasets we can create new `.csv` files that we can use for the rest of this notebook. \\\n",
    "\\\n",
    "We do this by using the built-in pandas function `pd.to_csv` that will automatically convert the pandas dataframe to a `.csv` file \\\n",
    "\\\n",
    "We will also create shortcuts to those files so the code becomes more human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pd.to_csv function is run by first specifying the file path and name that it should create.\n",
    "# The index = False argument specifies to the function that the .csv file should not include the index that the pandas dataframe has\n",
    "corona_df.to_csv(\"../Data/processed/de_corona.csv\", index = False)\n",
    "weather_df.to_csv(\"../Data/processed/de_weather.csv\", index = False)\n",
    "merged_df.to_csv(\"../Data/processed/de_corona_weather.csv\", index = False)\n",
    "\n",
    "# Now creating the shortcuts to the processed datasets\n",
    "corona_path = \"../Data/processed/de_corona.csv\"\n",
    "weather_path = \"../Data/processed/de_weather.csv\"\n",
    "merged_path = \"../Data/processed/de_corona_weather.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read in and overwrite the name variables of the datasets to make sure that the data we use has been filtered and cleaned correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df = pd.read_csv(corona_path)\n",
    "weather_df = pd.read_csv(weather_path)\n",
    "merged_df = pd.read_csv(merged_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Single variable analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframes for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dataframes with different time frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing values in date column to be datetime type\n",
    "merged_df[\"date\"] = pd.to_datetime(merged_df[\"date\"])\n",
    "\n",
    "#creating dataframes with dates and summed confirmed addition\n",
    "#all the dates\n",
    "merged_dfbydate=merged_df.groupby(by='date')['confirmed_addition'].sum().reset_index()\n",
    "#one date per week\n",
    "merged_dfbyweek=merged_dfbydate.groupby(pd.Grouper(key='date',freq='W')).sum().reset_index()\n",
    "#one date per month\n",
    "merged_dfbymonth=merged_dfbydate.groupby(pd.Grouper(key='date',freq='M')).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dataframes with weather data for different time frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create dataframes with weather data but only for some dates we will calculate the mean of the variable for that date\n",
    "#daily\n",
    "temp_dfbydate=merged_df.groupby(by='date').mean()\n",
    "temp_dfbydate=temp_dfbydate.loc[:,temp_dfbydate.columns!='confirmed_addition']\n",
    "merged_dfbydate=merged_dfbydate.merge(temp_dfbydate,on='date')\n",
    "#weekly\n",
    "temp_dfbyweek=merged_df.groupby(by='date').mean()\n",
    "temp_dfbyweek=temp_dfbyweek.loc[:,temp_dfbyweek.columns!='confirmed_addition']\n",
    "merged_dfbyweek=merged_dfbyweek.merge(temp_dfbyweek,on='date')\n",
    "#monthly\n",
    "temp_dfbymonth=merged_df.groupby(by='date').mean()\n",
    "temp_dfbymonth=temp_dfbymonth.loc[:,temp_dfbymonth.columns!='confirmed_addition']\n",
    "merged_dfbymonth=merged_dfbymonth.merge(temp_dfbymonth,on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dataframes with COVID-19 data for different time frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframes with only date, region_code and confirmed addition columns grouped by date and region_code\n",
    "#daily\n",
    "cor_dfbydate=merged_df.groupby(['date','region_code']).mean()\n",
    "cor_dfbydate.reset_index(inplace=True)\n",
    "cor_dfbydate=cor_dfbydate.loc[:,['date','region_code','confirmed_addition']]\n",
    "cormerged_dfbydate=cor_dfbydate.merge(merged_dfbydate.loc[:,'date'],on='date')\n",
    "#weekly\n",
    "cor_dfbyweek=merged_df.groupby(['date','region_code']).mean()\n",
    "cor_dfbyweek.reset_index(inplace=True)\n",
    "cor_dfbyweek=cor_dfbyweek.loc[:,['date','region_code','confirmed_addition']]\n",
    "cormerged_dfbyweek=cor_dfbyweek.merge(merged_dfbyweek.loc[:,'date'],on='date')\n",
    "#monthly\n",
    "cor_dfbymonth=merged_df.groupby(['date','region_code']).mean()\n",
    "cor_dfbymonth.reset_index(inplace=True)\n",
    "cor_dfbymonth=cor_dfbymonth.loc[:,['date','region_code','confirmed_addition']]\n",
    "cormerged_dfbymonth=cor_dfbymonth.merge(merged_dfbymonth.loc[:,'date'],on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the index to be the date column\n",
    "merged_df.set_index('date', inplace=True)\n",
    "cormerged_dfbyweek.set_index('date', inplace=True)\n",
    "cormerged_dfbymonth.set_index('date', inplace=True)\n",
    "cormerged_dfbydate.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting corona data\n",
    "After plotting corona data grouped by region. The plot was not readable and we decided that it will be better if we split the plot into 2 plots with 8 regions. We divided them according to the population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting corona data for all regions daily\n",
    "cormerged_dfbydate.groupby('region_code')['confirmed_addition'].plot(x=cormerged_dfbydate.index,kind=\"line\",legend=True, figsize=(15,13),title='Corona data daily',ylabel='Confirmed addition')\n",
    "plt.show()\n",
    "#Plotting corona data for all regions weekly\n",
    "cormerged_dfbyweek.groupby('region_code')['confirmed_addition'].plot(x=cormerged_dfbyweek.index,kind=\"line\",legend=True, figsize=(15,13),title='Corona data weekly',ylabel='Confirmed addition')\n",
    "plt.show()\n",
    "#Plotting corona data for all regions monthly\n",
    "cormerged_dfbymonth.groupby('region_code')['confirmed_addition'].plot(x=cormerged_dfbymonth.index,kind=\"line\",legend=True, figsize=(15,13),title='Corona data monthly',ylabel='Confirmed addition')\n",
    "plt.show()\n",
    "\n",
    "#Plotting corona data for 8 more populated German regions weekly\n",
    "newmerged_df=cormerged_dfbyweek.loc[(cormerged_dfbyweek['region_code']=='DE-SN')|(cormerged_dfbyweek['region_code']=='DE-RP')|(cormerged_dfbyweek['region_code']=='DE-NW')|(cormerged_dfbyweek['region_code']=='DE-NI')|(cormerged_dfbyweek['region_code']=='DE-HE')|(cormerged_dfbyweek['region_code']=='DE-BY')|(cormerged_dfbyweek['region_code']=='DE-BW')|(cormerged_dfbyweek['region_code']=='DE-BE')]\n",
    "newmerged_df.groupby('region_code')['confirmed_addition'].plot(x=newmerged_df.index,kind=\"line\",legend=True, figsize=(15,13),title='Corona data in the 8 more populated German regions weekly',ylabel='Confirmed addition')\n",
    "plt.show()\n",
    "#Plotting corona data for 8 less populated German regions weekly\n",
    "newmerged_df=cormerged_dfbyweek.loc[(cormerged_dfbyweek['region_code']=='DE-BB')|(cormerged_dfbyweek['region_code']=='DE-HB')|(cormerged_dfbyweek['region_code']=='DE-HH')|(cormerged_dfbyweek['region_code']=='DE-MV')|(cormerged_dfbyweek['region_code']=='DE-SL')|(cormerged_dfbyweek['region_code']=='DE-ST')|(cormerged_dfbyweek['region_code']=='DE-SH')|(cormerged_dfbyweek['region_code']=='DE-TH')]\n",
    "newmerged_df.groupby('region_code')['confirmed_addition'].plot(x=newmerged_df.index,kind=\"line\",legend=True, figsize=(15,13),title='Corona data in the 8 less populated German regions weekly',ylabel='Confirmed addition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting weather data\n",
    "We already created dataframes with weather data grouped by date with average values for all the weater variables.\n",
    "(RelativeHumiditySurface,Surfacepressure,TemperatureAboveGround,Totalprecipitation,UVIndex,WindSpeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we are plotting one subplot for each variable from the list:\n",
    "weather=[['relative_humidity_surface','surface_pressure','temperature_above_ground'],['total_precipitation','UV_index','wind_speed']]\n",
    "#We create figure for every time frequency  and then we plot 'date' column on X axis and weather data on Y axis\n",
    "#plotting weather variables daily\n",
    "fig, ax = plt.subplots(2,3,figsize=(23,13))\n",
    "fig.suptitle(' Weather data daily ', fontsize=20)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[i][j].plot(merged_dfbydate['date'],merged_dfbydate[weather[i][j]])\n",
    "        ax[i][j].set_title(weather[i][j])\n",
    "        ax[i][j].set_xlabel('date')\n",
    "#plotting weather variables weekly\n",
    "fig1, ax1 = plt.subplots(2,3,figsize=(23,13))\n",
    "fig1.suptitle(' Weather data weekly ', fontsize=20)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax1[i][j].plot(merged_dfbyweek['date'],merged_dfbyweek[weather[i][j]])\n",
    "        ax1[i][j].set_title(weather[i][j])\n",
    "        ax1[i][j].set_xlabel('date')\n",
    "#plotting weather variables monthly\n",
    "fig2, ax2 = plt.subplots(2,3,figsize=(23,13),num='Weather data monthly')\n",
    "fig2.suptitle(' Weather data monthly ', fontsize=20)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax2[i][j].plot(merged_dfbymonth['date'],merged_dfbymonth[weather[i][j]])\n",
    "        ax2[i][j].set_title(weather[i][j])\n",
    "        ax2[i][j].set_xlabel('date')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `RelativeHumiditySurface` we can see that there is an increase in the values throughout the year, they reach the highest values in the colder months, and then decrease drastically during March. This decrease is more difficult to notice in the monthly weather data plot, compared to the daily plot where the difference is showcased more clearly.\n",
    "\n",
    "While observing `Surfacepressure` we can mostly see similar values during the whole year with a few occasional falls. Again, if we look at the monthly weather data plot we can notice that the values are not as equally distributed since those falls change the mean of the pressure for that month.\n",
    "\n",
    "The graph for `TemperatureAboveGround` shows what typical temperatures for each season are, during the warmer months our values keep increasing, reaching their peak in August and then decreasing during the colder months to reach the lowest values in February.\n",
    "\n",
    "`Totalpercipitation` plot was the most challenging one to observe since the daily, weekly, and monthly weather data plots look dissimilar. If we look at the daily weather data plot, we can see that the values fluctuate drastically through all of the months reaching both low and high values. For this reason, we can see bigger differences in weekly and monthly means which would then explain the overall difference in graphs.\n",
    "\n",
    "`UVIndex` values can be compared to the `TemperatureAboveGround` values in the way that both of them are higher during warm months and lower during cold ones. The difference would be that `UVIndex` has a more steep line and therefore declines more rapidly to the low values, for example, after the peak that it reaches in August.\n",
    "\n",
    "Finally, `WindSpeed` fluctuates daily so we have numerous jumps that result in different means for weekly and monthly values. At the same time, we can notice that during winter we have overall greater values compared to the other seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling data from our datafile from OWID and merging it to our data frame.\n",
    "owid = pd.read_csv(\"../Data/external/de_our_world_in_data_covid.csv\")\n",
    "owid = owid[owid[\"iso_code\"] == \"DEU\"][['date', 'stringency_index']].dropna()\n",
    "df = pd.read_csv(\"../Data/processed/de_corona_weather.csv\")\n",
    "df = df.merge(owid, on=\"date\")\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "#Visualizing the stringency index.\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(df.index, df[\"stringency_index\"])\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=(1, 1)))\n",
    "plt.xticks(rotation = 45)\n",
    "fig.suptitle(\"Stringency Index for DE by month\", fontweight = \"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While researching whether there is a significant statistical association in Germany between weather data and infection rates, the most important variables that we took in consideration were:\n",
    "\n",
    "> `R-squared` (%): analyzes how much the changes in the independent variable can explain the changes in the dependent variable\n",
    ">\n",
    "> `Adj. R-squared` (%): is R-squared, but modified for the number of variables\n",
    ">\n",
    "> `coef`: measures how change in one variable affect the independent variable; if negative, our variables will have an inverse relationship, meaning that if one increases the other decreases\n",
    ">\n",
    "> `t`: measurement of the precision with which the coefficient is measured\n",
    ">\n",
    "> `p > |t|`: p value, based on the t statistic, shows how likely is our coefficient measured through our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables from the collected weather data\n",
    "Xs = ['relative_humidity_surface', 'solar_radiation', 'surface_pressure', 'temperature_above_ground', \n",
    "    'total_precipitation', 'UV_index', 'wind_speed']\n",
    "\n",
    "# Adding a constant term to both our dataframe and the Xs list\n",
    "merged_df = sm.add_constant(merged_df)\n",
    "Xs.append('const')\n",
    "\n",
    "# Performing the ordinary least squares regession to fit the regression line to our data\n",
    "est = sm.OLS(merged_df['confirmed_addition'], merged_df[Xs], hasconst=True).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Plotting weather variables with infection rates:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the UV index in comparison with confirmed cases.\n",
    "\n",
    "#Creating the plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "#Adding the first variable, UV Index\n",
    "ax.plot(df.index, df[\"UV_index\"], \"#fec44f\", label=\"UV Index\")\n",
    "\n",
    "#Adding a second variable and adding its own y-axis to show the confirmed additions\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(df.index, df[\"confirmed_addition\"], color =\"#2c7fb8\", label=\"Confirmed Addition\")\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=(1, 1)))\n",
    "\n",
    "#Adding the main title to the plot as well as the x and y labels\n",
    "fig.suptitle(\"UV Index for DE and number of confirmed cases\", fontweight = \"bold\")\n",
    "ax.set_ylabel(\"UV Index\", fontweight = \"bold\")\n",
    "ax2.set_ylabel(\"Confirmed addition\", fontweight = \"bold\")\n",
    "ax.set_xlabel(\"Date\", fontweight = \"bold\")\n",
    "\n",
    "#Adding the legend and specifying the position\n",
    "fig.legend(loc = \"upper left\", borderaxespad = 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the UV index with the amount of confirmed cases\n",
    "\n",
    "#Creating a separate dataframe to use when plotting that only contains the UV index and confirmed addition\n",
    "rel_df = merged_df.groupby(by = \"UV_index\" ) [\"confirmed_addition\"].mean().round().reset_index()\n",
    "\n",
    "#Rounding the UV Index to make it easier to plot as the UV Index values had a lot decimal values\n",
    "rel_df[\"UV_index\"] = rel_df[\"UV_index\"].round()\n",
    "\n",
    "#Log transforming the confirmed addition\n",
    "rel_df[\"confirmed_addition\"] = np.log(rel_df[\"confirmed_addition\"]+ 1)\n",
    "\n",
    "#Finally plotting the two values using the seaborn relationship plot function as a lineplot\n",
    "sns.relplot(data=rel_df, x = \"UV_index\", y = \"confirmed_addition\", kind= \"line\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Association report:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `0.263` value of `R-squared`, weather variables can be used to explain `26.3%` of cases. `Adj. R-squared` with the value of `0.262` is almost the same as `R-squared` since we are dealing with a small number of variables and thus `Ajd.  R-squared` can be ruled out as not relevant.\n",
    "\n",
    "We have a `significance threshold` of `0.05` which is used to determine whether a variable has a significant association with our dependent value, `confirmed_addition`. If the variables have a `P-value` less than this (`0.05`) we cannot deny that there could be a correlation. Out of `8` independent variables, `temperature_above_ground` is the only one that can be excluded, since its `P-value` is `0.133`. \n",
    "\n",
    "We have chosen to focus on `relative_humidity_surface` and `UV_index` since they have high spikes in both negative and positive correlations with the `confirmed_additions`. The coefficients level for `relative_humidity_surface` shows that when it increases there is also a rise in the number of COVID-cases, whereas for spikes of `UV_index` values, there is a related decrease in COVID-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Map Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make it easier to create multiple maps we define a function. \\\n",
    "In this function you can specify what condition you want to visualize along with what color scale to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for creating Folium maps with choropleths to make it easier \n",
    "# and more compact to create multiple maps for different variables.\n",
    "def choropleth(data, condition, color):\n",
    "    geo_json_path='../data/raw/shapefiles/de.geojson'\n",
    "    f = folium.Figure(width=1000, height=600)\n",
    "    m = folium.Map(location=[51.1657, 10.4515],zoom_start=6,crs='EPSG3857',\n",
    "    zoom_control=False, scrollWheelZoom=False,dragging=False,tiles=\"cartodbpositron\").add_to(f)\n",
    "\n",
    "    folium.Choropleth(\n",
    "        geo_data = geo_json_path,\n",
    "        name = condition,\n",
    "        data = data,\n",
    "        columns = [\"region_code\", condition],\n",
    "        key_on = \"properties.iso_3166_2\",\n",
    "        fill_color = color,\n",
    "        fill_opacity = 0.7,\n",
    "        line_opacity = 0.2,\n",
    "        legend_name = f\"{condition.upper().replace('_', ' ')}\",\n",
    "    ).add_to(m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Making a map of the UV index across the different regions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(merged_df, \"UV_index\", \"PuBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Making a map of the amount of COVID cases per region_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choropleth(merged_df, \"cases_pc\", \"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Open Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare Germany to Sweden as Sweden is the country that is the furthest away from Germany and also has a significantly different climate and geographical features \\\n",
    "So we just do the same things to manipulate the German data to the Sweden data, so we will not go into as much detail with comments as we did with Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_weather_df = pd.read_csv(\"../Data/raw/weather/weather.csv\")\n",
    "#Loading the sweden data as a pandas dataframe\n",
    "\n",
    "se_weather_df.rename(columns={\n",
    "    'date': 'date', \n",
    "    'iso3166-2': 'region_code',\n",
    "    'RelativeHumiditySurface': 'relative_humidity_surface', \n",
    "    'SolarRadiation': 'solar_radiation', \n",
    "    'Surfacepressure': 'surface_pressure', \n",
    "    'TemperatureAboveGround': 'temperature_above_ground', \n",
    "    'Totalprecipitation': 'total_precipitation', \n",
    "    'UVIndex': 'UV_index', \n",
    "    'WindSpeed': 'wind_speed'}, inplace=True)\n",
    "\n",
    "#Making a dataset that only contains weatherdata from Sweden\n",
    "se_weather_df = se_weather_df[se_weather_df['region_code'].str.startswith('SE')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f64b7b-96dc-44d0-8fd2-28cc14221ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_corona_df=pd.read_csv('../data/raw/corona/se_corona.csv',sep='\\t')\n",
    "with open('../data/raw/metadata/se_metadata.json','r', encoding=\"utf8\") as f:\n",
    "    country_metadata=json.load(f)\n",
    "\n",
    "region_map = {country_metadata[\"country_metadata\"][i][\"covid_region_code\"]: \n",
    "    country_metadata[\"country_metadata\"][i][\"iso3166-2_code\"] for i in range(len(country_metadata[\"country_metadata\"]))}\n",
    "se_corona_df[\"region_code\"] = se_corona_df[\"region_code\"].map(region_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25a73d-77e6-41df-8888-bdf4acce7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_merged_df = se_corona_df.merge(se_weather_df)\n",
    "#Reversing the dates so they start from 2020 and not 2021\n",
    "se_merged_df = se_merged_df.iloc[::-1]\n",
    "se_merged_df.set_index(\"date\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the UV index in comparison with confirmed cases.\n",
    "\n",
    "\n",
    "#Creating the plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "#Adding the first variable, UV Index\n",
    "ax.plot(se_merged_df.index, se_merged_df[\"UV_index\"], \"#fec44f\", label=\"UV Index\")\n",
    "\n",
    "#Adding a second variable and adding its own y-axis to show the confirmed additions\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(se_merged_df.index, se_merged_df[\"confirmed_addition\"], color =\"#2c7fb8\", label=\"Confirmed Addition\")\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=(1, 1)))\n",
    "\n",
    "#Adding the main title to the plot as well as the x and y labels\n",
    "fig.suptitle(\"UV Index for SE and number of confirmed cases\", fontweight = \"bold\")\n",
    "ax.set_ylabel(\"UV Index\", fontweight = \"bold\")\n",
    "ax2.set_ylabel(\"Confirmed addition\", fontweight = \"bold\")\n",
    "ax.set_xlabel(\"Date\", fontweight = \"bold\")\n",
    "\n",
    "#Adding the legend and specifying the position\n",
    "fig.legend(loc = \"upper left\", borderaxespad = 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the UV index in Sweden also has significant impact on the number of confirmed COVID-19 cases. \\\n",
    "It shows that there is a strong correlation between these two variables and that it is not affected by the geographical location of the country."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b989256eb56f7c452f8df4a64159a114113bfaae6a03bedf3a667d033260f0d6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
